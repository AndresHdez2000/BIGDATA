{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Data-Intensive Applications 1\n",
    "\n",
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|No|Question|Answer| \n",
    "|--|--|--|\n",
    "|1|||\n",
    "|2|||\n",
    "|3|||\n",
    "|4|||\n",
    "|5|||\n",
    "|6|||\n",
    "\n",
    "- 1 There is a set of rules in order to reduce the complexity, these rules to reduce complexity belongs to a class. Can you mention that class? Provide some examples of magaging complexity\n",
    "\n",
    "R.-Simplicity, explosion of the state space, tight coupling of modules, tangled dependencies, inconsistent naming and terminology, hacks aimed at solving performance problems, special-casing to work around issues elsewhere\n",
    "\n",
    "- 2 Imagine that you are working in a water fall, the data designer provides you the description of the loading of data to your system, what is the next step? Can you provide a question that shoud be aked to start with this step?\n",
    "\n",
    "R.-Description of the Performance.\n",
    "\n",
    " When you increase a load parameter and keep the system resources (CPU, memory, network bandwidth, etc.) unchanged, how is the performance of your system affected?\n",
    "\n",
    "When you increase a load parameter, how much do you need to increase the resources if you want to keep performance unchanged?\n",
    "\n",
    "- 3 Imagine that you have been called to solve a problem where you have been told that it returns corruped response, which set of tools would you use to solve this problem? Is it pertinent to say that is easy to solve and you have a quick solution?\n",
    "\n",
    "R.- No, as long as it seems to be a systematic foult in the software. the set of tools are: Carefully thinking about assumptions and interactions in the system; thorough testing; process isolation; allowing processes to crash and restart; measuring, monitoring, and analyzing system behavior in production\n",
    "\n",
    "- 4 Give a quick definition of Scalability, and now tbhat you know what we are talking about, would you say that scalability is given in 2 axis only? what is the general assuption you must have?\n",
    "\n",
    "R.- Is the term used to describe a system’s ability to cope with increased load. We most start thinking about the different ways we may have in order to provide our software with a wide scope of situations where it can load information, not only in those 2 axis\n",
    "\n",
    "- 5 There exists a property, which can be independent in some way, but no system can run without it. Can you guess which property am I talking about? What is the duty of the operations team?\n",
    " \n",
    "R.- Operability, and some of the duties are: \n",
    "\n",
    "        * Monitoring the health of the system and quickly restoring service if it goes into a bad state\n",
    "         \n",
    "        * Tracking down the cause of problems, such as system failures or degraded performance\n",
    "\n",
    "        * Keeping software and platforms up to date, including security patches\n",
    "\n",
    "\n",
    "- 6 Before getting along with growth questions, what do we need to do first? Which parameter would you use as reference to have a good quality in this part?\n",
    "\n",
    "\n",
    "R.-  we need to succinctly describe the current load on the system,  Load can be described with a few numbers which we call load parameters. The best choice of parameters depends on the architecture of your system: it may be requests per second to a web server, the ratio of reads to writes in a database, the number of simultaneously active users in a chat room, the hit rate on a cache, or something else. Perhaps the average case is what matters for you, or perhaps your bottleneck is dominated by a small number of extreme cases.\n",
    "\n",
    "- 7  Enlist the 3 design principles and give a definition to each one.\n",
    "\n",
    "R.- Operability\n",
    "    \n",
    "    * Make it easy for operations teams to keep the system running smoothly.\n",
    "Simplicity\n",
    "\n",
    "    * Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system. (Note this is not the same as simplicity of the user interface.)\n",
    "Evolvability\n",
    "\n",
    "     * Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibil‐ ity, modifiability, or plasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 1.- Describe impedance mismatch and give an example of it. Also find in which type of databases is present... \n",
    "\n",
    "R.-  SQL data model: if data is stored in application code and the database model of tables, rows, and columns.  The disconnect between the models is sometimes called an impedance mismatch\n",
    "\n",
    "* 2.- How many directions could a NoSQL database can have, name them, and define them.\n",
    "\n",
    "R.-\n",
    "         - 1. Document databases target use cases where data comes in self-contained documents and relationships between one document and another are rare.\n",
    "       \n",
    "        - 2. Graph databases go in the opposite direction, targeting use cases where anything is potentially related to everything.\n",
    "        \n",
    "* 3 Make a list whith the characteristics of a vertex, and an edge.\n",
    "\n",
    "R.-\n",
    "\n",
    "Each edge consists of:\n",
    "\n",
    "         - A unique identifier\n",
    "        - A set of outgoing edges\n",
    "        - A set of incoming edges\n",
    "        - A collection of properties (key-value pairs)\n",
    "\n",
    "Each edge consists of:\n",
    "\n",
    "        -A unique identifier\n",
    "        -The vertex at which the edge starts (the tail vertex)\n",
    "        -The vertex at which the edge ends (the head vertex)\n",
    "        -A label to describe the kind of relationship between the two vertices\n",
    "        - A collection of properties (key-value pairs)\n",
    "\n",
    "* 4 If you were told to create a social  network for your school, where the user may have interactions, and also, saving some files about the different relationships of each user, what Data Model would you use?\n",
    "\n",
    "R.- I would use an Graph-like Data Model. This because Social graphs, where, Vertices are people, and edges indicate which people know each other, could be very useful in order to reach the needs of this job. Well known algorithms can operate on these graphs: for example, car navigation systems search for the shortest path between two points in a road network. So all this tools are availables, in order to get important insights about the students who are using this network.\n",
    "\n",
    "* -5 Describe the two chracteristics of the triple store model. \n",
    "\n",
    "R.- \n",
    "\n",
    "        - 1. A value in a primitive datatype, such as a string or a number. In that case, the predicate and object of the triple are equivalent to the key and value of a property on the subject vertex. \n",
    "        \n",
    "        -2. Another vertex in the graph. In that case, the predicate is an edge in the graph, the subject is the tail vertex, and the object is the head vertex. \n",
    "        \n",
    "\n",
    "* -6 What are the most relevant aspects of graph model?\n",
    "\n",
    "R.- \n",
    "\n",
    "        -Any vertex can have an edge connecting it with any other vertex. There is no schema that restricts which kinds of things can or cannot be associated.\n",
    "        -Given any vertex, you can efficiently find both its incoming and its outgoing edges, and thus traverse the graph, follow a path through a chain of vertices both forward and backward. \n",
    "\n",
    "        -By using different labels for different kinds of relationships, you can store several different kinds of information in a single graph, while still maintaining a clean data model. Those features give "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* -1 If you were in 2050 and your co-worker is an android, what structure would you send to him? would it need desgine?\n",
    "\n",
    "R.- I would send him a log, because a log is used in the more general sense: an append-only sequence of records. It doesn’t have to be human-readable; it might be binary and intended only for other programs to read. My coworker doesn't need it to be nice documented, with a lot of desgine and Arial 12 APA style. \n",
    "\n",
    "* -2 What are indexers? Is data with indexers more efficient? \n",
    "\n",
    "R.- Indexes are  is an additional structure that is derived from the primary data. Many databases allow you to add and remove indexes, and this doesn’t affect the contents of the database; it only affects the performance of queries. Maintaining additional structures incurs overhead, especially on writes. For writes, it’s hard to beat the performance of simply appending to a file, because that’s the simplest possible write operation. Any kind of index usually slows down writes, because the index also needs to be updated every time data is written.\n",
    "\n",
    "* -3 Imagine you want to perform an implementation of a a hash index, which issues would you find on the way, give a brief explanation\n",
    "\n",
    "R.-\n",
    "\n",
    "    - File Format.- CSV is not the best format for a log. It’s faster and simpler to use a binary format that first encodes the length of a string in bytes, followed by the raw string\n",
    "    \n",
    "    -Deleting records.- If you want to delete a key and its associated value, you have to append a special deletion record to the data file \n",
    "    \n",
    "    - Crash recovery.- If the database is restarted, the in-memory hash maps are lost. In principle, you can restore each segment’s hash map by reading the entire segment file from beginning to end and noting the offset of the most recent value for every key as you go along. \n",
    "    \n",
    "    -Partially written records The database may crash at any time, including halfway through appending a record to the log. \n",
    "    \n",
    "* -4 What do you have to do in order to update the value of an existing key of a B-Tree?\n",
    "\n",
    "R.- You search for the leaf page containing that key, change the value in that page, and write the page back to disk (any references to that page remain valid). If you want to add a new key, you need to find the page whose range encompasses the new key and add it to that page. If there isn’t enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdvision of key ranges\n",
    "\n",
    "* -5 Explain what is a data warehousing and Wher could we use a data warehousing.\n",
    "\n",
    "R.-  is a separate database that analysts can query to their hearts’ content, without affecting OLTP operations. The data warehouse contains a read-only copy of the data in all the various OLTP systems in the company. Data is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. We can use a data warehouse in an enterprise with dozens of different transaction processing systems: systems powering the customer-facing website, controlling point of sale (checkout) systems in physical stores, tracking inventory in warehouses, planning routes for vehicles, managing suppliers, administering employees, etc. Each of these systems is complex and needs a team of people to maintain it, so the systems end up operating mostly autonomously from each other\n",
    " \n",
    "* -6 Describe a user facing system, and explain its features\n",
    "\n",
    "R.- OLTP systems are typically user-facing, which means that they may see a huge volume of requests. In order to handle the load, applications usually only touch a small number of records in each query. The application requests records using some kind of key, and the storage engine uses an index to find the data for the requested key. Disk seek time is often the bottleneck here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "\n",
    "\n",
    "* -1 What elements should be able whenever we have a schemas or format changing\n",
    "\n",
    "R.-  \n",
    "\n",
    "        -Backward compatibility.- Newer code can read data that was written by older code.\n",
    "        -Forward compatibility.- Older code can read data that was written by newer code.\n",
    "\n",
    "* -2 Define the two different representations of how programswork with data\n",
    "\n",
    "R.- \n",
    "\n",
    "        -In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and soon. These data structures are optimized for efficient access and manipulation by the CPU (typically using pointers).\n",
    "        \n",
    "        -When you want to write data to a file or send it over the network, you have to encode it as some kind of self-contained sequence of bytes (for example, a JSON document). \n",
    "\n",
    "* -3 Describe at least 3 different encoding formats \n",
    "\n",
    "R.-\n",
    "\n",
    "        -Programming language–specific encodings are restricted to a single programming language and often fail to provide forward and backward compatibility.\n",
    "        \n",
    "        -Textual formats like JSON, XML, and CSV are widespread, and their compatibility depends on how you use them. They have optional schema languages, which are sometimes helpful and sometimes a hindrance. These formats are somewhat vague about datatypes, so you have to be careful with things like numbers and binary strings.\n",
    "        \n",
    "        - Binary schema–driven formats like Thrift, Protocol Buffers, and Avro allow compact, efficient encoding with clearly defined forward and backward compatibility semantics. The schemas can be useful for documentation and code generation in statically typed languages. However, they have the downside that data needs to be decoded before it is human-readable.\n",
    "\n",
    "    \n",
    "* -4 Describe some scencarios where the models of dataflow and data encoding are important\n",
    "\n",
    "R.\n",
    "\n",
    "        -Databases, where the process writing to the database encodes the data and the process reading from the database decodes it\n",
    "        \n",
    "        - RPC and REST APIs, where the client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response\n",
    "        \n",
    "        -Asynchronous message passing (using message brokers or actors), where nodes communicate by sending each other messages that are encoded by the sender and decoded by the recipient\n",
    "\n",
    "* -5 Define the actor model, and mention where is it used the programming modeldistributed actor framework\n",
    "\n",
    "R.-The actor model is a programming model for concurrency in a single process. Rather than dealing directly with threads (and the associated problems of race conditions, locking, and deadlock), logic is encapsulated in actors. Each actor typically represents one client or entity, it may have some local state (which is not shared with any other actor), and it communicates with other actors by sending and receiving asynchronous messages. Message delivery is not guaranteed: in certain error scenarios, messages will be lost. Since each actor processes only one message at a time, it doesn’tneed to worry about threads, and each actor can be scheduled independently by the framework.framework. It is used to scale an application across multiple nodes.\n",
    "\n",
    "* -6 There is a distributed actor frmawork which handle message encoding which is developed in a concurrent program which we saw in programming paradigms, give the features, and add to other frameworks \n",
    "\n",
    "R.- \n",
    "\n",
    "        -Erlang OTP. It is surprisingly hard to make changes to record schemas (despite the system having many features designed for high availability); rolling upgrades are possible but need to be planned carefully. An experimental new maps datatype (a JSON-like structure, introduced in Erlang R17 in 2014) may make this easier in the future. \n",
    "        \n",
    "        -Akka uses Java’s built-in serialization by default, which does not provide forward or backward compatibility. However, you can replace it with something like Protocol Buffers, and thus gain the ability to do rolling upgrades.\n",
    "        \n",
    "        -Orleans by default uses a custom data encoding format that does not support rolling upgrade deployments; to deploy a new version of your application, you need to set up a new cluster, move traffic from the old cluster to the new one, and shut down the old one. Like with Akka, custom serialization plug-ins can be used.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 5\n",
    "\n",
    "|No.|Question|Answer|\n",
    "|--|--|--|\n",
    "|1|Why would you replicate the data?| * To keep data geographically close to your users<br>* To allow the system to continue working even if some of its parts have failed<br>* To scale out the number of machines that can serve read queries|\n",
    "|2|Define the Leader-Based replication|has one major downside: there is only one leader, and all writes must go through it. If you can’t connect to the leader for any reason, for example due to a network interruption between you and the leader, you can’t write to the database | \n",
    "|3|Imaging that you are deploying a replication of your data at Islas Tikiti, which configuration would you used, knowing that you get short on internet very frequently?|I would use a Multi-Leader-Based replication because Traffic between datacenters usually goes over the public internet, which may beless reliable than the local network within a datacenter. A single-leader configuration is very sensitive to problems in this inter-datacenter link, because writes are made synchronously over this link. A multi-leader configuration with asynchronous replication can usually tolerate network problems better: a temporary network interruption does not prevent writes being processed.|\n",
    "|4|Now lets say that you are just selling something not that big, just for you to sell hotwings |You should use a single leader replication, the replication steel happneds and the process of configuration is much more easy and you are supposed to have control over the employees in another sucursal in case of froud |\n",
    "|5|Define the two ways your code could be executed in a multi-leader- replication whenever is a conflict|* __On write__  As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler. For example, Bucardo allows you to write a snippet of Perl for this purpose. This handler typically cannot prompt a user, it runs in a background process and it must execute quickly. <br>* __On read__ When a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict, and write the result back to the database|\n",
    "|6|Give some characteristics of leaderless replication| the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes. As we shall see, this difference in design has profound consequences for the way the database is used.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 6\n",
    "\n",
    "|No|Question|Answer| \n",
    "|--|--|--|\n",
    "|1|Mention some of the properties that change in a database through time|• The query throughput increases, so you want to add more CPUs to handle the load. <br>• The dataset size increases, so you want to add more disks and RAM to store it.<br> • A machine fails, and other machines need to take over the failed machine’s responsibilities.| \n",
    "|2|Give a description of the two different approaches of partitioning|* __Key range partitioning__, where keys are sorted, and a partition owns all the keys from some minimum up to some maximum. Sorting has the advantage that efficient range queries are possible, but there is a risk of hot spots if the application often accesses keys that are close together in the sorted order<br>* __Hash partitioning__, where a hash function is applied to each key, and a partition owns a range of hashes. This method destroys the ordering of keys, making range queries inefficient, but may distribute load more evenly.\n",
    "|\n",
    "|3|Mention one of the most significant objectives with partitioning |Spread the data and the query load evenly across nodes. If every node takes a fair share, then—in theory—10 nodes should be able to handle 10 times as much data and 10 times the read and write throughput of a single node|\n",
    "|4|Mention an advangae of dynamic partitioning|The number of partitions adapts to the total data volume. If there is only a small amount of data, a small number of partitions is sufficient, so overheads are small; if there is a huge amount of data, the size of each individual partition is limited to a configurable maximum|\n",
    "|5|How can we choose the right configuration of partitions?|Choosing the right number of partitions is difficult if the total size of the dataset is highly variable. Since each partition contains a fixed fraction of the total data, the size of each partition grows proportionally to the total amount of data in the cluster. If partitions are very large, rebalancing and recovery from node failures become expensive. But if partitions are too small, they incur too much overhead. The best performance is achieved when the size of partitions is “just right,” neither too big nor too small, which can be hard to achieve if the number of partitions is fixed but the dataset size varies.|\n",
    "|6|Why the fully automated balancing can be convinient| Because there is less operational work to do for normal maintenance. However, it can be unpredictable. Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7\n",
    "\n",
    "|No|Question|Answer| \n",
    "|--|--|--|\n",
    "|1|How does durability works in single-node databases?| It usually also involves a write-ahead log or similar, which allows recovery in the event that the data structures on disk are corrupted. In a replicated database, durability may mean that the data has been successfully copied to some number of nodes. In order to provide a durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed.|\n",
    "|2|Describe the meaning of the A and the I from the ACID acronym|__Atomicity__ If an error occurs halfway through a sequence of writes, the transaction should be aborted, and the writes made up to that point should be discarded. In other words, the database saves you from having to worry about partial failure, by giving an all-or-nothing guarantee. <br>__Isolation__ Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.|\n",
    "|3|Provide a list where you can explain where the retrying an aborted transaction is not good at all.| If the transaction actually succeeded, but the network failed while the server tried to acknowledge the successful commit to the client , then retrying the transaction causes it to be performed twice—unless you have an additional application-level deduplication mechanism in place.<br>• If the error is due to overload, retrying the transaction will make the problem worse, not better. To avoid such feedback cycles, you can limit the number of retries, use exponential backoff, and handle overload-related errors differently from other errors . <br>• It is only worth retrying after transient errors ; after a perma‐ nent error a retry would be pointless. <br>• If the transaction also has side effects outside of the database, those side effects may happen even if the transaction is aborted. For example, if you’re sending an email, you wouldn’t want to send the email again every time you retry the transaction. If you want to make sure that several different systems either commit or abort together, two-phase commit can help . <br>• If the client process fails while retrying, any data it was trying to write to the database is lost|\n",
    "|4|Give the garantees of _read commited_|1. When reading from the database, you will only see data that has been committed (no dirty reads). <br>2. When writing to the database, you will only overwrite data that has been committed (no dirty writes).|\n",
    "|5|Give examples of concurrency problems that aisolation can avoid| IF a used car sales website on which two people, Alice and Bob, are simultaneously trying to buy the same car. Buying a car requires two database writes: the listing on the website needs to be updated to reflect the buyer, and the sales invoice needs to be sent to the buyer.The sale is awarded to Bob (because he performs the winning update to the listings table), but the invoice is sent to Alice (because she performs the winning update to the invoices table). Read committed prevents such mishaps. <br>• However, read committed does not prevent the race condition between two counter increments. In this case, the second write happens after the first transaction has committed, so it’s not a dirty write. It’s still incorrect, but for a different reason—in “Preventing Lost Updates”.| \n",
    "|6|Describe de two developments that cause concurrency problems mainly. |RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory (see “Keeping everything in memory” on page 88). When all data that a transaction needs to access is in memory, transactions can execute much faster than if they have to wait for data to be loaded from disk.<br> • Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes. By contrast, long-running analytic queries are typically readonly, so they can be run on a consistent snapshot (using snapshot isolation) outside of the serial execution loop.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
